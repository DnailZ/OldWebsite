---
layout: simple
title:  "POPL-24 Artifact Evalation - Yuantian Ding"
---

# POPL-24 Artifact Evaluation - Yuantian Ding

## Claims in the paper

* Biten outperforms CVC4, Probe, Duet, and Simba on all the 3 benchmark sets.
* Biten generated smaller solution for Case-splitting and Deobfuscation Benchmark
* Ablation Studies:
    * LLM-guidance and Bottom-up Deduction can effectively increase the solving rate of Hacker's Delight Benchmark.
    * Bottom-up Deduction and Graph-based enumeration can increase the performance for Case-splitting Benchmark.
    * Bottom-up Deduction can increase the solving rate for Deobfuscation Benchmark.

Note: Biten is a name for our anonymous submission. Biten is actually part of our general SyGuS solver DryadSynth.

## Download, and installation

The only file you need to download is `BitenArifact.ova`, you can download this file from [OneDrive](https://purdue0-my.sharepoint.com/:u:/g/personal/ding360_purdue_edu/EeAL4PYsLThDkEdLB_iLOLsBS0_6oVxFqA659VS0RPz89g?e=eTOsw1).

The image can be loaded by VirtualBox 7. We recommend using at least 4 CPUs and at least 8Â GB memory. 

After the virtual machine (VM) is up and running, you can access it via SSH using the following credentials: `username: virtualbox` and `password: changeme` on localhost at port 2222. For a seamless experience in exploring the figures generated by test scripts, we suggest using [VS Code Remote SSH](https://code.visualstudio.com/docs/remote/ssh) to connect to the VM.

All related files are under the directory `/home/virtualbox/artifact`. Everything is installed in the VM, no need to run any compilation.

Here is the structure of `/home/virtualbox/artifact` Directory:

```
artifact/
    scripts/    all scripts you need to run the test
    benchmarks/     All 3 benchmark sets
    solvers/    4 solvers used in the test
    clean.sh    remove all files generated during testing 
    README.md   a copy this file (without OneDrive link)
```

## Evaluation instructions

### Hacker's Delight Benchmark

Solving hacker's delight benchmarks involves interaction with a large language model. So please set up your **OpenAI API key** for `gpt-turbo-3.5`. To test the effectiveness of LLM-guided enumeration, please set `OPENAI_API_KEY` in the environment variable:

```bash
export OPENAI_API_KEY=<your key>
```

Use the following to run all the Hacker's Delight Benchmark (Ablation Study included) for all solvers and all configurations. It will be pretty slow because OpenAI have rate limits about 150,000 token per minutes. If we run Biten multiple times continueously, ChatGPT will reach its limit. So the testing program will `sleep` some time to work around this limit.

```bash
python3 scripts/bench.py hd-run
```

All results are generated in `json` file under `results/`. The procedure may be halted during execution, but any completed test results will be preserved within the `results/` folder. To resume the process, simply execute `scripts/bench.py hd-run` once more, and any previously tested results will be skipped. If you wish to rerun the entire test, you can achieve this by deleting the `results` directory, and then running `scripts/bench.py hd-run` again.

Generate figures for each configuration:

```bash
python3 scripts/bench.py hd-draw
```

`hd-1.png`, `hd-2.png`, `hd-dc-1.png`, `hd-dc-2.png` will be created in the working directory.

### Case-splitting PBE Benchmark


Run all the Case-splitting PBE benchmarks for all solvers and all configurations:

```bash
python3 scripts/runbench.py run
```

All results are generated in `json` file under `result0/`. This command shares similar semantics with `scripts/bench.py hd-run`.

Generate all data in CSV using the following command:

```bash
python3 scripts/bench.py to-csv
```

It will compose all `json` data into `results.csv`. It will also generate figure `sygus-time.png`, `sygus-size.png` which compare each solver on time and size.

### Deobfuscation Benchmark

Run all the Deobfuscation benchmarks for all solvers and all configurations:

```bash
python3 scripts/runbench.py run-deobfusc
```

All results are generated in `json` file under `result1/`. This command shares similar semantics with `scripts/bench.py hd-run`. 

Generate all data in CSV with the following command:

```bash
python3 scripts/runbench.py to-csv-2
```

It will compose all JSON data into `results1.csv`. It will also generate figure `deobfusc-time.png`, `deobfusc-size.png` which compare each solver on time and size.


`python3 scripts/runbench.py draw-scatter` will draw ablation study figures for Bottom-up deduction and Graph-based Enumeration. There are 4 figures generated from this command:

* `sygus-deduct.png`: Case-splitting benchmark W/ and W/o Bottom-up Deduction
* `sygus-graph.png`: Case-splitting benchmark W/ and W/o Graph-based Enumeration
* `deobfusc-deduct.png`: Deobfuscation benchmark W/ and W/o Bottom-up Deduction
* `deobfusc-graph.png`: Deobfuscation benchmark W/ and W/o Graph-based Enumeration

### Testing a Single Benchmark Manually

Biten's executable is located at `solvers/biten/biten`. A TOML file can be passed into `biten` for configuration using the following command:

```bash
biten -c <config-file> <sygus-if-file>
```

In `solvers/biten/config`, there are a lot of commonly used configuration files. Here is the default configuration file for PBE problems with explanation of each parameter:

```toml
random_example = 10 # How many additional random examples are generated from reference implementation 
limited_ite = false # Limit ITE condition search size to 10000 
ite_tree_limit = 30 # Limit the size of decition tree
chatgpt = true # Disable/Enable ChatGPT
smt_solver = "bitwuzla" # Using command bitwuzla as SMT-Solver. Require `bitwuzla` command installed

[expr_search]
sample = 64 # The number of sampling
dag_size = true # Enable Graph-Based Enumeration
filter = {
    deductive_combine = true, # Enable/Disable deduction for AND and OR
    deductive_reverse = true  # Enable/Disable deduction for XOR and ADD
} 
```

`-v` flag can be passed into `biten` command to display log information.


## Additional Information

* Testing all benchmarks in one benchmark set for all solvers and all configurations will normally cost **6 hours or more**. For faster testing, consider change `TIMEOUT` variable in `./script/bench.py` and `./script/runbench.py` to a smaller value (in seconds).
* To check if the benchmarks run as expected, we provide a list of our experimental results in our [Appendix](https://dnailz.github.io/assets/appendix.pdf). Please manually execute the `biten` command again to verify if the error is **temporary**, possibly caused by an OpenAI rate limit or an out-of-memory (OOM) kill.
* You can look up any specific result in the `results/` folder, each `json` file is in the following format:
    ```
    {
        "status": <"success" if the command returns 0, otherwise it could be "error" or "timeout">
        "stdout": <STDOUT>,
        "stderr": <STDERR>,
        "time": <time for the execution>
        "command": <command for the execution>,
    }
    ```
* For questions, please email me at **ding360@purdue.edu**. I will be happy to help you with any problems.
